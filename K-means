#Import necessary libraries:

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
numpy and pandas are libraries for numerical operations and data manipulation.
TfidfVectorizer from sklearn.feature_extraction.text is used to convert text documents into TF-IDF (Term Frequency-Inverse Document Frequency) vectors.
KMeans from sklearn.cluster is used to perform K-Means clustering.
cosine_similarity from sklearn.metrics.pairwise is used to compute the cosine similarity between vectors.
nltk and nltk.corpus are Natural Language Toolkit libraries for text processing.
stopwords are common words that are removed from text data during preprocessing.
Define a list of text documents:
documents = [ "Text document 1", "Text document 2", ... ]
This list contains a collection of text documents that you want to cluster.

Define the number of clusters (K):
K = 2
This variable determines how many clusters the K-Means algorithm will attempt to create.

Remove stop words from the documents:
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

filtered_documents = []
for document in documents:
    words = document.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    filtered_documents.append(" ".join(filtered_words))
The nltk.download('stopwords') line downloads the NLTK stopwords for English.
stopwords.words('english') contains a set of common English stopwords like "the," "is," "in," etc.
The code removes these stopwords from each document and stores the filtered documents in the filtered_documents list.
TF-IDF Vectorization:
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(filtered_documents)
TfidfVectorizer is used to convert the filtered documents into TF-IDF vectors. It assigns a numerical value to each word in the documents, reflecting its importance in the context of the entire corpus.
X is the TF-IDF matrix representing the documents.
Initialize variables:
n_docs, n_features = X.shape
prev_labels = np.zeros(n_docs)
max_iterations = 100
n_docs is the number of documents, and n_features is the number of features (unique words) in the TF-IDF matrix.
prev_labels is initialized to zero and is used to store cluster labels from the previous iteration.
max_iterations sets a limit on the number of iterations for convergence.
Loop through the clustering process:
for _ in range(max_iterations):
This loop performs the iterative clustering process.

Initialize K-Means clustering:
kmeans = KMeans(n_clusters=K, random_state=0)
labels = kmeans.fit_predict(X)
A new instance of KMeans with K clusters is created, and documents are assigned to clusters (labels) based on the TF-IDF features.
Calculate cluster centers:
cluster_centers = np.zeros((K, n_features))
for i in range(K):
    mask = (labels == i)
    if np.any(mask):
        cluster_centers[i] = X[mask].mean(axis=0)
This block computes the cluster centers by taking the mean of the TF-IDF values of all documents assigned to each cluster.

Calculate similarities using cosine similarity:
similarities = cosine_similarity(X, cluster_centers)
Cosine similarity is calculated between each document and the cluster centers. It measures the cosine of the angle between TF-IDF vectors and is used to assess similarity.

Assign documents to the nearest clusters:
new_labels = np.argmax(similarities, axis=1)
The documents are assigned to the cluster with the highest cosine similarity.

Check for convergence:
if np.array_equal(new_labels, prev_labels):
    break
Convergence is checked by comparing the current cluster labels with the previous ones. If they are the same, the clustering process stops.

Update prev_labels:
prev_labels = new_labels
The cluster labels from the current iteration become the "previous" labels for the next iteration.

Clusters are now matched:
cluster_ids = new_labels.tolist()
The cluster_ids variable contains the final cluster assignments for each document.

Print the cluster assignments:
print(cluster_ids)
The code prints the cluster assignments to the console. These are the cluster numbers that each document belongs to.

The code essentially performs iterative K-Means clustering using cosine similarity to determine the closest cluster. It repeats this process until converg*ence or the maximum number of iterations is reached, and it assigns documents to clusters based on the calculated similarities.
